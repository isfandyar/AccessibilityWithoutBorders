{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Isfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Isfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "    # getting stemmer\n",
    "\n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "\n",
    "def my_tokenizer(sentence):\n",
    "\n",
    "        sentence = nltk.word_tokenize(sentence) \n",
    "\n",
    "        listofwords=[word.lower() for word in sentence if word.isalpha()]\n",
    "\n",
    "        listofstemmed_words = []\n",
    "\n",
    "        # remove stopwords and any tokens that are just empty strings\n",
    "        for word in listofwords:\n",
    "            if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "                # Stem words\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "        return listofstemmed_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('Model1_NLP.sav', 'rb'))\n",
    "loaded_tfidf = pickle.load(open('tfidf.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(txt, model):\n",
    "    sample_transformed = loaded_tfidf.transform(txt).toarray()\n",
    "    prediction = model.predict(sample_transformed)\n",
    "    prediction_probability = model.predict_proba(sample_transformed)\n",
    "    print(f\"Prediction: {prediction[0]}, Prediction Score: {np.max(prediction_probability)}\")\n",
    "    \n",
    "    # gets a dictionary of {'class_name': probability}\n",
    "    prob_per_class_dictionary = dict(zip(model.classes_, prediction_probability[0]))\n",
    "    return prob_per_class_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Input Text:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This demo is very cool and accessibility is important\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prediction:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: happy, Prediction Score: 0.7581012023521327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'anger': 0.05855381404491542,\n",
       " 'fear': 0.059965801965226245,\n",
       " 'happy': 0.7581012023521327,\n",
       " 'sad': 0.0504624586471826,\n",
       " 'surprise': 0.07291672299054301}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printmd(\"**Input Text:**\")\n",
    "text = input()\n",
    "text = [text]\n",
    "printmd(\"**Prediction:**\")\n",
    "predict_emotion(text,loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I feel so sad ']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Capstone-NLP)",
   "language": "python",
   "name": "capstone-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
