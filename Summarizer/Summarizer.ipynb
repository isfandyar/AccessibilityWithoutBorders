{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Isfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Isfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                        \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \n",
    "                        \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                        \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                        \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \n",
    "                        \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n",
    "                        \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \n",
    "                        \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                        \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                        \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "                        \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "                        \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                        \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \n",
    "                        \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
    "                        \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "                        \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \n",
    "                        \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \n",
    "                        \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                        \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                        \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                        \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                        \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                        \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "                        \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                        \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                        \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                        \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n",
    "                        \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                        \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                        \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                        \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                        \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \n",
    "                        \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                        \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions\n",
    "\n",
    "def summarize(ranked_sentences, length):\n",
    "    summary = \"\"\n",
    "    for i in range(length):\n",
    "        summary += ranked_sentences[i][1] + \" \"\n",
    "\n",
    "    return summary\n",
    "\n",
    "def rank(text, scores):\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(text)), reverse=True)\n",
    "    ranked = ranked_sentences\n",
    "        \n",
    "    return ranked\n",
    "\n",
    "def glove(cleaned_text):\n",
    "    glove_vectors = []\n",
    "    for i in cleaned_text:\n",
    "        if len(i)!=0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "         \n",
    "        glove_vectors.append(v)\n",
    "        \n",
    "    return glove_vectors\n",
    "\n",
    "def clean(text):\n",
    "    cleaned = []\n",
    "    for y in text:\n",
    "        cleaned.append(h_cleaner(y))\n",
    "        \n",
    "    return cleaned\n",
    "\n",
    "def ready(text):\n",
    "    cleaned = []\n",
    "    for x in text:\n",
    "        cleaned.append(s_cleaner(x))\n",
    "        \n",
    "    return cleaned\n",
    "\n",
    "def s_cleaner(text): \n",
    "    final_string=[]\n",
    "    for i in text:\n",
    "        new_string = re.sub(\"\\n\", \" \", text) \n",
    "        new_string = re.sub(\"Ã‚\\xad\", \"\",new_string)\n",
    "        new_string = re.sub(r'\\([^)]*\\)', '', new_string)\n",
    "        new_string = re.sub(r'\\[[^0-9]*\\]', '', new_string)\n",
    "        final_string.append(new_string)\n",
    "\n",
    "    return new_string\n",
    "\n",
    "def h_cleaner(text):   \n",
    "    new_string = text.lower()\n",
    "    new_string = re.sub(r'\\([^)]*\\)', '', new_string)\n",
    "    new_string = re.sub('\"','', new_string)\n",
    "    new_string = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_string.split(\" \")])    \n",
    "    new_string = re.sub(r\"'s\\b\",\"\",new_string)\n",
    "    new_string = re.sub(\"[^a-zA-Z]\", \" \", new_string) \n",
    "    new_string = [w for w in new_string.split() if not w in stop_words]\n",
    "    new_string = \" \".join(new_string)\n",
    "\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Orignal Text:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Joe Biden decamped to Wilmington on Friday, escaping the White House he's likened to a \"gilded cage\" for a weekend at his Delaware home. It's yet another weekend away from Washington as the President has spent a noteworthy portion of his first year in office at one of his two Delaware homes or at Camp David, working remotely.  A CNN analysis of Biden's public schedule indicates the President has spent a significant amount of time away from the White House, particularly on weekends, since his January inauguration. Including this weekend's trip to Delaware, Biden has taken 35 personal trips and spent all or part of 108 of his first 276 days in office at one of his Delaware homes or at Camp David in Maryland. That includes partial days, like Friday -- when he spent the day at the White House and departed in the evening. Sixty-nine of those 108 days away from Washington were spent at his home in Wilmington, spread over 23 visits; seven days at his Rehoboth Beach, Delaware, beach house over two visits; and 32 days at Camp David over 10 visits. Typically, his chopper commute from the White House to his Wilmington home takes less than an hour, and a White House official noted that many of the partial days included Biden departing the White House at the conclusion of a normal workday on a Friday or returning to Washington before the start of a workday on a Monday. That puts Biden ahead of the pace set by former President Donald Trump, who had spent less time at his Florida and New Jersey getaways at the same point in his presidency than Biden has spent in Delaware.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Summary:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's yet another weekend away from Washington as the President has spent a noteworthy portion of his first year in office at one of his two Delaware homes or at Camp David, working remotely. Including this weekend's trip to Delaware, Biden has taken 35 personal trips and spent all or part of 108 of his first 276 days in office at one of his Delaware homes or at Camp David in Maryland. Typically, his chopper commute from the White House to his Wilmington home takes less than an hour, and a White House official noted that many of the partial days included Biden departing the White House at the conclusion of a normal workday on a Friday or returning to Washington before the start of a workday on a Monday. \n"
     ]
    }
   ],
   "source": [
    "printmd(\"**Orignal Text:**\")\n",
    "text = input()\n",
    "sentences = sent_tokenize(text)\n",
    "r = ready(sentences)\n",
    "c = clean(sentences)\n",
    "word_embeddings = {}\n",
    "with open('dataset/glove/glove.6B.100d.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        word_embeddings[word] = coefs\n",
    "g = glove(c)\n",
    "sim_mat=np.zeros([len(g), len(g)])\n",
    "for y in range(len(g)):\n",
    "    for x in range(len(g)):\n",
    "        if y!=x:\n",
    "            sim_mat[y][x] = cosine_similarity(g[y].reshape(1,100), g[x].reshape(1,100))[0,0]\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "pagerank_scores = nx.pagerank_numpy(nx_graph)\n",
    "textranked = rank(r, pagerank_scores)\n",
    "textrank_summary = summarize(textranked, 3)\n",
    "print('\\n')\n",
    "printmd(\"**Summary:**\")\n",
    "print(textrank_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Capstone-NLP)",
   "language": "python",
   "name": "capstone-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
